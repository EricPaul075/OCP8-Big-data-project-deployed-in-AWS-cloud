{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "499ca33d-2a61-45f4-bebd-073b601551f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:47:26.799149Z",
     "iopub.status.busy": "2022-10-24T16:47:26.798920Z",
     "iopub.status.idle": "2022-10-24T16:48:11.066100Z",
     "shell.execute_reply": "2022-10-24T16:48:11.065313Z",
     "shell.execute_reply.started": "2022-10-24T16:47:26.799123Z"
    },
    "tags": []
   },
   "source": [
    "# Projet 8 - <font color='green'>Notebook</font> - Déployez un modèle dans le cloud\n",
    "Dans ce projet, nous mettons en œuvre une architecture et une application big data dans le but de construire un moteur de classification d'images de fruits.\n",
    "Le [jeu de données](https://www.kaggle.com/moltean/fruits) comprend des images de différents fruits sous différents angles.\n",
    "La mission consiste à prendre en considération le volume important de données pour réaliser une application en pyspark à exécuter dans le cloud sur une architecture big data.\n",
    "L'architecture big data est mise en place sur AWS en utilisant S3 pour le stockage des données et un cluster EMR pour l'exécution de l'application.\n",
    "L'application réalise l'acquisition des images et leur prétraitement comprenant l'extraction des features et la réduction de dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef5948f-aead-4435-b3a3-e8c2e9cd7b2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:47:26.799149Z",
     "iopub.status.busy": "2022-10-24T16:47:26.798920Z",
     "iopub.status.idle": "2022-10-24T16:48:11.066100Z",
     "shell.execute_reply": "2022-10-24T16:48:11.065313Z",
     "shell.execute_reply.started": "2022-10-24T16:47:26.799123Z"
    },
    "tags": []
   },
   "source": [
    "<a id=\"toc\"></a>\n",
    "**SOMMAIRE**\n",
    "\n",
    "-  [Préparation du cluster - architecture](#cluster)\n",
    "-  [Session spark](#session)\n",
    "-  [Configuration d'exécution](#config)\n",
    "-  [Lecture des données d'entrée](#read)\n",
    "-  [Fonctions de création du modèle CNN et d'extraction des features](#cnn)\n",
    "-  [Extraction itérative des features des images](#features)\n",
    "-  [Enregistrement du dataset](#dataset)\n",
    "-  [Standardisation des features de X](#standardization)\n",
    "-  [Réduction de dimension avec un PCA](#pca)\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e237ac8-05e9-40e5-a8d1-488f06e2b5b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:47:26.799149Z",
     "iopub.status.busy": "2022-10-24T16:47:26.798920Z",
     "iopub.status.idle": "2022-10-24T16:48:11.066100Z",
     "shell.execute_reply": "2022-10-24T16:48:11.065313Z",
     "shell.execute_reply.started": "2022-10-24T16:47:26.799123Z"
    },
    "tags": []
   },
   "source": [
    "<a id=\"cluster\"></a>\n",
    "### Préparation du cluster - architecture\n",
    "\n",
    "Le cluster cible consiste en une architecture Big Data EMR sur AWS avec les caractéristiques suivantes:\n",
    "- Localisation dans la région eu-west-3 (Paris) afin que la proximité minimise le temps de latence.\n",
    "- Gestion de la sécurité:\n",
    "  - Compte IAM mis en place, dont les crédendials figurent dans le fichier USER/.aws/credentials pour l'exécution en mode local (PC Windows 10 en stand alone).\n",
    "  - Paire de clés EC2 créée au prélable sur la région.\n",
    "  \n",
    "  &nbsp;\n",
    "- Version EMR 6.8.0, la plus récente, afin de disposer des versions les plus récentes des libraires Hadoop 3.2.1 et spark 3.3.0.\n",
    "- Afin d'exécuter ce notebook, les librairies JupyterEnterpriseGateway 2.1.0 et Livy 0.7.1 sont également incluses ; le noyau à utiliser avec jupyterLab est 'pyspark'.\n",
    "- L'architecture du cluster est basée sur des instances uniformes à usage général et constituée de :\n",
    "  - <font color='green'>1 nœud maitre</font> : m5.xlarge constitué de 4 cœurs virtuels, 16GB de RAM et 64 GB de stockage local (EBS) ;\n",
    "  - <font color='green'>2 nœuds esclaves</font>, dont pour chacune de ces instances d'exécution: m5.xlarge constitué de 4 cœurs virtuels, 16GB de RAM et 64 GB de stockage local (EBS).\n",
    "  \n",
    "  Notons que cette architecture est surdimensionnée pour les seules opérations de ce notebook, mais qu'elle correspond à la plus petite configuration à usage général disponible sur la région et permettra de traiter un nombre important d'images avec un nœud maitre pouvant disposant d'un volume mémoire important pour les opérations de sérialisation et des nœud d'exécution évolutifs par mise à l'échelle horizontale.\n",
    " \n",
    "- OS Amazon Linux (red hat).\n",
    "- Amorçage du cluster pour installer des librairies complémentaires sur chaque instance (fichier bootstrap.sh).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eee48d-5eae-476b-8a12-860465625eb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:47:26.799149Z",
     "iopub.status.busy": "2022-10-24T16:47:26.798920Z",
     "iopub.status.idle": "2022-10-24T16:48:11.066100Z",
     "shell.execute_reply": "2022-10-24T16:48:11.065313Z",
     "shell.execute_reply.started": "2022-10-24T16:47:26.799123Z"
    },
    "tags": []
   },
   "source": [
    "*[retour SOMMAIRE](#toc)*\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d075fa-ffbe-473f-9698-14ef1e436d2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:47:26.799149Z",
     "iopub.status.busy": "2022-10-24T16:47:26.798920Z",
     "iopub.status.idle": "2022-10-24T16:48:11.066100Z",
     "shell.execute_reply": "2022-10-24T16:48:11.065313Z",
     "shell.execute_reply.started": "2022-10-24T16:47:26.799123Z"
    },
    "tags": []
   },
   "source": [
    "<a id=\"session\"></a>\n",
    "### Session spark\n",
    "\n",
    "Il s'agit de la création de la SparkSession en fonction de l'architecture cible du cluster AWS, en spécifiant certains paramètres importants de configuration, puis utilisation du sparkContext pour régler le niveau de verbosité de l'application.\n",
    "S'agissant d'un notebook, l'initialisation de la session Spark s'effectue avec Apache Livy et permet de spécifier certaines configurations.\n",
    "\n",
    "Les paramètres de [configuration](https://spark.apache.org/docs/3.3.0/configuration.html#content):\n",
    "- '<font color='green'>spark.driver.maxResultSize</font>' est fixé par défaut à 1GB, ce qui peut devenir insuffisant pour effectuer des traitements nécessitant la sérialisation de gros dataframes (ex: PCA avec un grand nombre de features) ;\n",
    "- '<font color='green'>spark.driver.memory</font>' peut être à régler pour permettre au driver de disposer de suffisamment de mémoire pour sérialiser (collect) les données. Sa valeur maximum est limitée à la mémoire maximum disponible pour spark, fixée en particulier par 'spark.memory.fraction', dont la valeur par défaut est 0.6. Nous recherchons pour ce projet une valeur maximale afin de tirer partie de la RAM disponible pour gérer un dataset aussi volumineux que possible et minimiser le temps de sérialisation.\n",
    "&nbsp;\n",
    "- '<font color='green'>spark.executor.memory</font>' et '<font color='green'>spark.executor.cores</font>', valeurs par instance (nœud d'exécution), qui dépend du nombre de cœurs et de la RAM disponible par exécuteur. Nous recherchons une valeur relativement modeste pour 'spark.executor.memory' afin que lors d'une mise à l'échelle nous puissions maximiser la parallélisation en fonction du nombre de vcpu disponibles.\n",
    "\n",
    "**Note pour un fonctionnement sans Apache Livy** (ex: ligne de commande spark-submit): certaines valeurs (deploy) comme \"spark.driver.maxResultSize\" et \"spark.driver.memory\" ne peuvent pas être modifiées dans l'application, ce qui rend nécessaire de les spécifier dans 'SPARK_HOME/conf/spark-defaults.conf' ou au lancement de l'application avec la ligne de commande 'spark-submit --conf spark.driver.maxResultSize=0 main.py' (si l'application est lancée depuis le script python main.py). Dans l'environnement EMR avec AWS linux, la modification de ce paramètre s'effectue sur le nœud maitre avec 'sudo vim /etc/spark/conf/spark-defaults.conf' en ajoutant 'spark.driver.maxResultSize  0'. La valeur '0' permet de ne pas limiter sa valeur, avec le risque de rencontrer une erreur si la taille d'une donnée sérialisée sur le nœud maitre fait dépasser sa capacité mémoire ; l'alternative consisterait à lui donner une valeur fixée, par exemple la même que celle donnée à 'spark.driver.memory'.\n",
    "Si le fichier 'spark-defaults.conf' n'existe pas encore :\n",
    "- *sur le cluster AWS*, exécuter un premier 'spark-submit' crée ce fichier qu'on peut ensuite modifier ,\n",
    "- *en local*, l'installation de pyspark avec pip n'installe pas ce fichier et il faut créer le répertoire 'conf' et y placer le fichier 'spark-defaults.conf' ;\n",
    "\n",
    "&nbsp;\n",
    "Nous configurons par ailleurs 'Apache arrow' ainsi que recommandé pour l'utilisation de Pandas et Numpy et de vectorisation UDFs (User Defined Functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af39e5a4-9bc2-4183-9b7e-e6e5ed113dc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:44:47.719918Z",
     "iopub.status.busy": "2022-10-28T12:44:47.719710Z",
     "iopub.status.idle": "2022-10-28T12:44:47.736297Z",
     "shell.execute_reply": "2022-10-28T12:44:47.735666Z",
     "shell.execute_reply.started": "2022-10-28T12:44:47.719892Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.driver.cores': '1', 'spark.driver.memory': '8g', 'spark.driver.maxResultSize': '0', 'spark.executor.instances': '2', 'spark.executor.cores': '4', 'spark.executor.memory': '1g'}, 'proxyUser': 'user_ep075', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"conf\":{\"spark.driver.cores\": \"1\",\n",
    "         \"spark.driver.memory\": \"8g\",\n",
    "         \"spark.driver.maxResultSize\": \"0\",\n",
    "         \"spark.executor.instances\": \"2\",\n",
    "         \"spark.executor.cores\": \"4\",\n",
    "         \"spark.executor.memory\": \"1g\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520d68bd-a6e9-46f7-b99d-bb9cf2467f44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:44:47.753657Z",
     "iopub.status.busy": "2022-10-28T12:44:47.753488Z",
     "iopub.status.idle": "2022-10-28T12:45:25.620032Z",
     "shell.execute_reply": "2022-10-28T12:45:25.619337Z",
     "shell.execute_reply.started": "2022-10-28T12:44:47.753635Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069d9758fccd4db9b0591e39fd57d61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>6</td><td>application_1666957824279_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-16-49.eu-west-3.compute.internal:20888/proxy/application_1666957824279_0007/\" class=\"emr-proxy-link\" emr-resource=\"j-2LZZUEGKCB2WU\n",
       "\" application-id=\"application_1666957824279_0007\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-22-36.eu-west-3.compute.internal:8042/node/containerlogs/container_1666957824279_0007_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cr?ation d'une session spark avec la configuration:\n",
      "('spark.eventLog.enabled', 'true')\n",
      "('spark.driver.extraJavaOptions', \"-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -XX:OnOutOfMemoryError='kill -9 %p'\")\n",
      "('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES', 'http://ip-172-31-16-49.eu-west-3.compute.internal:20888/proxy/application_1666957824279_0007')\n",
      "('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS', 'ip-172-31-16-49.eu-west-3.compute.internal')\n",
      "('spark.sql.parquet.output.committer.class', 'com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter')\n",
      "('spark.blacklist.decommissioning.timeout', '1h')\n",
      "('spark.yarn.appMasterEnv.SPARK_PUBLIC_DNS', '$(hostname -f)')\n",
      "('spark.sql.emr.internal.extensions', 'com.amazonaws.emr.spark.EmrSparkSessionExtensions')\n",
      "('spark.eventLog.dir', 'hdfs:///var/log/spark/apps')\n",
      "('spark.executor.memory', '1g')\n",
      "('spark.driver.maxResultSize', '0')\n",
      "('spark.yarn.heterogeneousExecutors.enabled', 'false')\n",
      "('spark.history.fs.logDirectory', 'hdfs:///var/log/spark/apps')\n",
      "('spark.ui.filters', 'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter')\n",
      "('spark.app.id', 'application_1666957824279_0007')\n",
      "('spark.repl.class.uri', 'spark://ip-172-31-22-36.eu-west-3.compute.internal:39553/classes')\n",
      "('spark.executor.extraLibraryPath', '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native')\n",
      "('spark.hadoop.yarn.timeline-service.enabled', 'false')\n",
      "('spark.yarn.tags', 'livy-session-6-WTnEYuHw')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.driver.extraClassPath', '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar')\n",
      "('spark.repl.class.outputDir', '/mnt1/yarn/usercache/livy/appcache/application_1666957824279_0007/container_1666957824279_0007_01_000001/tmp/spark1109907853991171219')\n",
      "('spark.app.name', 'livy-session-6')\n",
      "('spark.executorEnv.PYTHONPATH', '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9.5-src.zip')\n",
      "('spark.yarn.dist.jars', 'file:///usr/lib/livy/rsc-jars/arpack_combined_all-0.1.jar,file:///usr/lib/livy/rsc-jars/core-1.1.2.jar,file:///usr/lib/livy/rsc-jars/jniloader-1.1.jar,file:///usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/native_ref-java-1.1.jar,file:///usr/lib/livy/rsc-jars/native_system-java-1.1.jar,file:///usr/lib/livy/rsc-jars/netlib-native_ref-linux-armhf-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_ref-linux-i686-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_ref-linux-x86_64-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_ref-osx-x86_64-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_ref-win-i686-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_ref-win-x86_64-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_system-linux-armhf-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_system-linux-i686-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_system-linux-x86_64-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_system-osx-x86_64-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_system-win-i686-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netlib-native_system-win-x86_64-1.1-natives.jar,file:///usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar,file:///usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar,file:///usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar,file:///usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar')\n",
      "('spark.hadoop.mapreduce.output.fs.optimized.committer.enabled', 'true')\n",
      "('spark.decommissioning.timeout.threshold', '20')\n",
      "('spark.sql.catalogImplementation', 'hive')\n",
      "('spark.yarn.dist.pyFiles', '')\n",
      "('spark.stage.attempt.ignoreOnDecommissionFetchFailure', 'true')\n",
      "('spark.hadoop.fs.s3.getObject.initialSocketTimeoutMilliseconds', '2000')\n",
      "('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem', '2')\n",
      "('spark.yarn.app.id', 'application_1666957824279_0007')\n",
      "('spark.submit.deployMode', 'cluster')\n",
      "('spark.app.startTime', '1666961108517')\n",
      "('spark.yarn.submit.waitAppCompletion', 'false')\n",
      "('spark.yarn.dist.files', 'file:/etc/hudi/conf.dist/hudi-defaults.conf')\n",
      "('spark.yarn.app.container.log.dir', '/var/log/hadoop-yarn/containers/application_1666957824279_0007/container_1666957824279_0007_01_000001')\n",
      "('spark.executor.cores', '4')\n",
      "('spark.driver.host', 'ip-172-31-22-36.eu-west-3.compute.internal')\n",
      "('spark.sql.warehouse.dir', 'hdfs://ip-172-31-16-49.eu-west-3.compute.internal:8020/user/spark/warehouse')\n",
      "('spark.yarn.dist.archives', 'file:/usr/lib/spark/R/lib/sparkr.zip#sparkr')\n",
      "('spark.yarn.maxAppAttempts', '1')\n",
      "('spark.executor.instances', '2')\n",
      "('spark.sql.hive.metastore.sharedPrefixes', 'com.amazonaws.services.dynamodbv2')\n",
      "('spark.driver.memory', '8g')\n",
      "('spark.app.attempt.id', '1')\n",
      "('spark.yarn.secondary.jars', 'arpack_combined_all-0.1.jar,core-1.1.2.jar,jniloader-1.1.jar,livy-api-0.7.1-incubating.jar,livy-rsc-0.7.1-incubating.jar,livy-thriftserver-session-0.7.1-incubating.jar,native_ref-java-1.1.jar,native_system-java-1.1.jar,netlib-native_ref-linux-armhf-1.1-natives.jar,netlib-native_ref-linux-i686-1.1-natives.jar,netlib-native_ref-linux-x86_64-1.1-natives.jar,netlib-native_ref-osx-x86_64-1.1-natives.jar,netlib-native_ref-win-i686-1.1-natives.jar,netlib-native_ref-win-x86_64-1.1-natives.jar,netlib-native_system-linux-armhf-1.1-natives.jar,netlib-native_system-linux-i686-1.1-natives.jar,netlib-native_system-linux-x86_64-1.1-natives.jar,netlib-native_system-osx-x86_64-1.1-natives.jar,netlib-native_system-win-i686-1.1-natives.jar,netlib-native_system-win-x86_64-1.1-natives.jar,netty-all-4.1.17.Final.jar,commons-codec-1.9.jar,livy-core_2.12-0.7.1-incubating.jar,livy-repl_2.12-0.7.1-incubating.jar')\n",
      "('spark.sql.parquet.fs.optimized.committer.optimization-enabled', 'true')\n",
      "('spark.driver.cores', '1')\n",
      "('spark.driver.port', '39553')\n",
      "('spark.driver.extraLibraryPath', '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native')\n",
      "('spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem', 'true')\n",
      "('spark.livy.spark_major_version', '3')\n",
      "('spark.executor.extraClassPath', '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar')\n",
      "('spark.history.ui.port', '18080')\n",
      "('spark.shuffle.service.enabled', 'true')\n",
      "('spark.driver.defaultJavaOptions', \"-XX:OnOutOfMemoryError='kill -9 %p'\")\n",
      "('spark.resourceManager.cleanupExpiredHost', 'true')\n",
      "('spark.executor.defaultJavaOptions', \"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\")\n",
      "('spark.app.submitTime', '1666961090551')\n",
      "('spark.files.fetchFailure.unRegisterOutputOnHost', 'true')\n",
      "('spark.master', 'yarn')\n",
      "('spark.ui.port', '0')\n",
      "('spark.emr.default.executor.memory', '9486M')\n",
      "('spark.executor.extraJavaOptions', \"-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\")\n",
      "('spark.yarn.historyServer.address', 'ip-172-31-16-49.eu-west-3.compute.internal:18080')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.dynamicAllocation.enabled', 'true')\n",
      "('spark.yarn.isPython', 'true')\n",
      "('spark.emr.default.executor.cores', '4')\n",
      "('spark.blacklist.decommissioning.enabled', 'true')"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('ocp8') \\\n",
    "    .getOrCreate()\n",
    "prev = spark.conf.get(\n",
    "    \"spark.sql.execution.arrow.pyspark.enabled\")  # get previous conf\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\",\n",
    "               True)  # Arrow optimization for pyspark.sql.DataFrame\n",
    "conf = spark.sparkContext.getConf().getAll()\n",
    "print(\"Création d'une session spark avec la configuration:\")\n",
    "for item in conf:\n",
    "    print(item)\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020a88c-89c2-4671-a05b-24eb6af952eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:47:26.799149Z",
     "iopub.status.busy": "2022-10-24T16:47:26.798920Z",
     "iopub.status.idle": "2022-10-24T16:48:11.066100Z",
     "shell.execute_reply": "2022-10-24T16:48:11.065313Z",
     "shell.execute_reply.started": "2022-10-24T16:47:26.799123Z"
    },
    "tags": []
   },
   "source": [
    "*[retour SOMMAIRE](#toc)*\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5f8d3-86aa-486b-bd7f-027a67c171e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:47:26.799149Z",
     "iopub.status.busy": "2022-10-24T16:47:26.798920Z",
     "iopub.status.idle": "2022-10-24T16:48:11.066100Z",
     "shell.execute_reply": "2022-10-24T16:48:11.065313Z",
     "shell.execute_reply.started": "2022-10-24T16:47:26.799123Z"
    },
    "tags": []
   },
   "source": [
    "<a id=\"config\"></a>\n",
    "### Configuration d'exécution\n",
    "\n",
    "Utilisation de 2 constantes pour spécifier cette configuration :\n",
    "- Mode local (PC windows 10 en stand alone) vs le mode d'exécution sur AWS EMR\n",
    "- Mode debug: dans ce mode, le nombre d'images et de features sont réduits pour exécuter rapidement l'application et investiguer notamment des configurations de gestion mémoire du cluster (nota: la configuration matérielle des instances permet d'aller jusqu'à au moins 10000 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ee58df-c553-4a9e-9686-b0d510e55a2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:45:25.621305Z",
     "iopub.status.busy": "2022-10-28T12:45:25.621131Z",
     "iopub.status.idle": "2022-10-28T12:45:25.679156Z",
     "shell.execute_reply": "2022-10-28T12:45:25.678376Z",
     "shell.execute_reply.started": "2022-10-28T12:45:25.621283Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cb0f9ff1be4813bcd40065a6a71ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex?cution du script en local=False et debug=False"
     ]
    }
   ],
   "source": [
    "# Configuration d'exécution\n",
    "LOCAL = False  # False si exécution sur aws EMR\n",
    "DEBUG = False  # Mode debug\n",
    "print(f\"Exécution du script en local={LOCAL} et debug={DEBUG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c4c0f5-3456-4dda-a58f-112d36a69c63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:48:37.513695Z",
     "iopub.status.busy": "2022-10-24T16:48:37.513454Z",
     "iopub.status.idle": "2022-10-24T16:48:38.298286Z",
     "shell.execute_reply": "2022-10-24T16:48:38.297516Z",
     "shell.execute_reply.started": "2022-10-24T16:48:37.513667Z"
    },
    "tags": []
   },
   "source": [
    "*[retour SOMMAIRE](#toc)*\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48fb9a-5f8e-4ca1-ad06-227aebaa95cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:48:37.513695Z",
     "iopub.status.busy": "2022-10-24T16:48:37.513454Z",
     "iopub.status.idle": "2022-10-24T16:48:38.298286Z",
     "shell.execute_reply": "2022-10-24T16:48:38.297516Z",
     "shell.execute_reply.started": "2022-10-24T16:48:37.513667Z"
    },
    "tags": []
   },
   "source": [
    "<a id=\"read\"></a>\n",
    "### Lecture des données d'entrée\n",
    "\n",
    "Les images sont stockées sur S3, compartiment \"ocp8project\", répertoire \"input_data/\" et chaque fruit fait l'objet d'un répertoire à son nom contenant les images de ce fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "000d978d-25d9-4871-ab2a-f336ee1f997a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:45:25.681564Z",
     "iopub.status.busy": "2022-10-28T12:45:25.681304Z",
     "iopub.status.idle": "2022-10-28T12:45:26.456229Z",
     "shell.execute_reply": "2022-10-28T12:45:26.455217Z",
     "shell.execute_reply.started": "2022-10-28T12:45:25.681529Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd12b36d26e4402b59040a24ddcc4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure de stockage des donn?es dans S3:\n",
      "AWS_S3_BUCKET=ocp8project\n",
      "INPUT_DATA_FOLDER=input_data/\n",
      "OUTPUT_DATA_FOLDER=output_data/\n",
      "Nombre d'objets dans s3://ocp8project/input_data/: 91"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Structure de stockage du projet dans S3\n",
    "print(\"Structure de stockage des données dans S3:\")\n",
    "AWS_S3_BUCKET = \"ocp8project\"\n",
    "print(f\"AWS_S3_BUCKET={AWS_S3_BUCKET}\")\n",
    "INPUT_DATA_FOLDER = \"input_data/\"\n",
    "print(f\"INPUT_DATA_FOLDER={INPUT_DATA_FOLDER}\")\n",
    "OUTPUT_DATA_FOLDER = \"output_data/\"\n",
    "print(f\"OUTPUT_DATA_FOLDER={OUTPUT_DATA_FOLDER}\")\n",
    "\n",
    "# Création d'un client pour l'accès à S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# Lecture des données d'entrées\n",
    "objects = s3_client.list_objects_v2(Bucket=AWS_S3_BUCKET,\n",
    "                                    Prefix=INPUT_DATA_FOLDER)\n",
    "n_objects = objects['KeyCount']\n",
    "print(\n",
    "    f\"Nombre d'objets dans s3://{AWS_S3_BUCKET}/{INPUT_DATA_FOLDER}: {n_objects}\"\n",
    ")\n",
    "if n_objects <= 1:\n",
    "    print(\"Erreur: le répertoire des données d'entrée est vide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc996fc-1688-48d7-9bd1-8ec7c9f68fe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:48:50.597839Z",
     "iopub.status.busy": "2022-10-24T16:48:50.597622Z",
     "iopub.status.idle": "2022-10-24T16:48:52.906666Z",
     "shell.execute_reply": "2022-10-24T16:48:52.905988Z",
     "shell.execute_reply.started": "2022-10-24T16:48:50.597815Z"
    },
    "tags": []
   },
   "source": [
    "*[retour SOMMAIRE](#toc)*\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d25c94-54d3-47d6-b84d-0470dfb8fcca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:48:50.597839Z",
     "iopub.status.busy": "2022-10-24T16:48:50.597622Z",
     "iopub.status.idle": "2022-10-24T16:48:52.906666Z",
     "shell.execute_reply": "2022-10-24T16:48:52.905988Z",
     "shell.execute_reply.started": "2022-10-24T16:48:50.597815Z"
    },
    "tags": []
   },
   "source": [
    "<a id=\"cnn\"></a>\n",
    "### Fonctions de création du modèle CNN et d'extraction des features\n",
    "\n",
    "Nous utilisons le modèle EfficientNet pour extraire les features avec du transfer-learning, en utilisant la totalité du modèle à l'exception de la couche de classification.\n",
    "Nous y ajoutons une couche 'GlobalAveragePooling2D' afin de réduire chaque feature de la dimension 7*7 à 1. Le nombre de features est donc de 1280.\n",
    "La fonction d'extraction des features de chaque image utilise en entrée l'image telle que chargée par PIL et produit en sortie un vecteur pyspark.ml.linalg.Vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ceb93ff-0fee-4eec-ab00-77e8a60a42a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:45:26.458068Z",
     "iopub.status.busy": "2022-10-28T12:45:26.457900Z",
     "iopub.status.idle": "2022-10-28T12:45:28.752864Z",
     "shell.execute_reply": "2022-10-28T12:45:28.752238Z",
     "shell.execute_reply.started": "2022-10-28T12:45:26.458047Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7646b6df0df43deb19169690984de3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.applications import EfficientNetB0\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def get_model_from_EfficientNetB0(layer='top_activation'):\n",
    "    \"\"\"\n",
    "    Contruit le modèle basé sur EfficientNetB0 jusqu'à la couche\n",
    "        spécifiée, selon le niveau attendu des features à extraire,\n",
    "        puis ajoute une couche GlobalAveragePooling2D pour prendre\n",
    "        la valeur moyenne (de la matrice 7*7) de chaque feature.\n",
    "    :param layer: str, nom de la dernière couche keras layer du\n",
    "        modèle EfficientNetB0 à utiliser, par défaut la dernière\n",
    "        couche avant celle de classification.\n",
    "    :return: keras.models.Model, modèle produisant les features.\n",
    "    \"\"\"\n",
    "    # Modèle de base EfficientNet sans la couche de classification\n",
    "    base_model = EfficientNetB0(weights=\"imagenet\",\n",
    "                                include_top=False,\n",
    "                                input_shape=(224, 224, 3))\n",
    "    #print(base_model.summary())  # Donne les noms de chaque layer\n",
    "\n",
    "    # Sélection du modèle jusqu'à une couche spécifique selon le niveau  attendu des features\n",
    "    x = base_model.get_layer(layer).output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "from keras.utils import load_img, img_to_array\n",
    "from keras.applications.efficientnet import preprocess_input\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "def feature_extraction(model,\n",
    "                       img_path=None,\n",
    "                       img=None,\n",
    "                       debug=False,\n",
    "                       debug_feat_size=10):\n",
    "    \"\"\"\n",
    "    Extrait les features de l'image avec le modèle (1280 features avec\n",
    "        EfficientNetB0).\n",
    "    :param model: keras.models.Model, modèle produisant les features,\n",
    "        issu de la fonction get_model_from_EfficientNetB0.\n",
    "    :param img_path: str, chemin de l'image dont les features seront\n",
    "        extraites par la fonction.\n",
    "    :param img: PIL image au format (224, 224).\n",
    "    :param debug: bool, mode debug, default=False.\n",
    "    :param debug_feat_size: int, dimension du vecteur de sortie pour\n",
    "        le mode debug, défault=10.\n",
    "    :return: pyspark.ml.linalg.Vectors, vecteur 1D des features de\n",
    "        l'image.\n",
    "    \"\"\"\n",
    "    if img_path is not None:\n",
    "        # Charge et redim filtre par défaut\n",
    "        img = load_img(img_path, target_size=(224, 224), keep_aspect_ratio=True)\n",
    "    elif img is None:\n",
    "        print('Aucune image spécifiée en entrée')\n",
    "        return None\n",
    "    img = img_to_array(img)  # Conversion de l'image en np.array\n",
    "    img = img.reshape(\n",
    "        (1, img.shape[0], img.shape[1], img.shape[2]))  # Reshape format CNN\n",
    "    img = preprocess_input(img)  # Preprocessing efficientnet\n",
    "    # Prédiction + reshape 1D + format liste + Vectorisation\n",
    "    features = Vectors.dense(model.predict(img).ravel().tolist()[:debug_feat_size]) \\\n",
    "        if debug else Vectors.dense(model.predict(img).ravel().tolist())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e5545-1c78-41bc-ac57-58b41d8c44d9",
   "metadata": {},
   "source": [
    "*[retour SOMMAIRE](#toc)*\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7477b04-5acb-43da-90c9-298446db2661",
   "metadata": {},
   "source": [
    "<a id=\"features\"></a>\n",
    "### Extraction itérative des features des images\n",
    "\n",
    "L'extraction s'effectue image par image selon le processus :\n",
    "- Lecture de chaque fichier de INPUT_DATA_FOLDER: label et image du fruit ;\n",
    "- Codage des labels dans un dictionnaire {\"nom_fruit\": int} ;\n",
    "- Pour chaque fichier image (filename!=''), chargement de l'image et affichage de son nom, format et dimensions ;\n",
    "- Redimensionnement de l'image à la dimension d'entrée du modèle ;\n",
    "- Création d'un dataframe contenant le label de l'image ;\n",
    "- Création d'un dataframe contenant le vecteur des features de l'image ;\n",
    "- Concaténation progressive dans un dataframe y (labels) et un dataframe X (features).\n",
    "\n",
    "En mode DEBUG, ce processus se limite à 'max_img' images et 'max_feat' features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e47c47f-0674-4f72-bc87-0ba68bfaf1ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:45:28.754141Z",
     "iopub.status.busy": "2022-10-28T12:45:28.753944Z",
     "iopub.status.idle": "2022-10-28T12:45:50.106809Z",
     "shell.execute_reply": "2022-10-28T12:45:50.106000Z",
     "shell.execute_reply.started": "2022-10-28T12:45:28.754118Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49cd73d3e0da4c559a450397185a00b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture des images et extraction des features avec le CNN 'EfficientNetB0':\n",
      "Corn: 0_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Corn: 199_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "Corn: r2_123_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Corn: r2_12_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "Corn: r2_135_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "Corn: r2_147_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "Corn: r2_182_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Corn: r2_194_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Corn: r2_24_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "Corn: r2_36_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "Corn: r2_48_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "Corn: r2_87_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "Corn: r2_99_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "Corn: r_0_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "Corn: r_100_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "Corn: r_119_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Corn: r_129_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Corn: r_136_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "Corn: r_169_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Corn: r_175_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Corn: r_188_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Corn: r_18_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "Corn: r_29_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Corn: r_30_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "Corn: r_41_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "Corn: r_42_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Corn: r_69_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Corn: r_6_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "Corn: r_75_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Corn: r_83_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Mandarine: 0_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Mandarine: 120_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "Mandarine: 132_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Mandarine: 163_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Mandarine: 190_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Mandarine: 273_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Mandarine: 299_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Mandarine: 311_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Mandarine: 323_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Mandarine: 60_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Mandarine: 96_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Mandarine: r_122_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Mandarine: r_146_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Mandarine: r_158_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Mandarine: r_170_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Mandarine: r_182_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Mandarine: r_19_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Mandarine: r_206_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Mandarine: r_218_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Mandarine: r_230_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Mandarine: r_242_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Mandarine: r_268_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Mandarine: r_280_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Mandarine: r_292_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Mandarine: r_306_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Mandarine: r_317_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Mandarine: r_31_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Mandarine: r_327_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Mandarine: r_43_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Mandarine: r_7_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Raspberry: 0_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Raspberry: 118_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Raspberry: 11_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Raspberry: 130_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Raspberry: 142_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Raspberry: 166_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Raspberry: 178_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Raspberry: 190_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "Raspberry: 23_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Raspberry: 250_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Raspberry: 262_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Raspberry: 274_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Raspberry: 286_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Raspberry: 310_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Raspberry: 322_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Raspberry: 35_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Raspberry: 59_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Raspberry: 71_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Raspberry: r_145_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Raspberry: r_157_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Raspberry: r_169_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "Raspberry: r_181_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "Raspberry: r_193_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Raspberry: r_222_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Raspberry: r_249_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Raspberry: r_286_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Raspberry: r_327_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Raspberry: r_48_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "Raspberry: r_6_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Raspberry: r_84_100.jpg, JPEG, (100, 100), RGB\n",
      "1/1 [==============================] - 0s 52ms/step"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "print(\n",
    "    \"Lecture des images et extraction des features avec le CNN 'EfficientNetB0':\"\n",
    ")\n",
    "labels = dict()\n",
    "label_count = 0\n",
    "img_count = 0\n",
    "max_img = 10 if DEBUG else n_objects  # remplacer 'n_objects' par une constante selon la capacité du cluster\n",
    "max_feat = 10 if DEBUG else 1280\n",
    "model = get_model_from_EfficientNetB0()\n",
    "\n",
    "for obj in objects['Contents']:\n",
    "    label = obj['Key'].split('/')[-2]\n",
    "    filename = obj['Key'].split('/')[-1]\n",
    "    if filename != '':\n",
    "\n",
    "        # Codage du label\n",
    "        if label not in labels.keys():\n",
    "            labels.update({label: label_count})\n",
    "            label_count += 1\n",
    "\n",
    "        # Lecture et redimensionnement de l'image\n",
    "        file = s3_client.get_object(Bucket=AWS_S3_BUCKET, Key=obj['Key'])\n",
    "        img = Image.open(file['Body'])\n",
    "        print(f\"{label}: {filename}, {img.format}, {img.size}, {img.mode}\")\n",
    "        if img.size != (224, 224):\n",
    "            img = img.resize((224, 224))\n",
    "\n",
    "        # labels et features des images\n",
    "        sdf_label = spark.createDataFrame([(labels[label],)], ['label'])\n",
    "        features = feature_extraction(model,\n",
    "                                      img=img,\n",
    "                                      debug=DEBUG,\n",
    "                                      debug_feat_size=max_feat)\n",
    "        sdf_features = spark.createDataFrame([(features,)], ['features'])\n",
    "        if img_count == 0:\n",
    "            y = sdf_label\n",
    "            X = sdf_features\n",
    "            features_size = len(features)\n",
    "        else:\n",
    "            y = y.union(sdf_label)\n",
    "            X = X.union(sdf_features)\n",
    "        img_count += 1\n",
    "    if img_count == max_img:\n",
    "        break  # mode debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed8452c-a38f-4654-98ac-6ee91ec690c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:49:52.351193Z",
     "iopub.status.busy": "2022-10-24T16:49:52.350957Z",
     "iopub.status.idle": "2022-10-24T16:50:29.834018Z",
     "shell.execute_reply": "2022-10-24T16:50:29.832936Z",
     "shell.execute_reply.started": "2022-10-24T16:49:52.351166Z"
    },
    "tags": []
   },
   "source": [
    "*[retour SOMMAIRE](#toc)*\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92cc58b-43e9-409b-a680-16e60d05dacd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:49:52.351193Z",
     "iopub.status.busy": "2022-10-24T16:49:52.350957Z",
     "iopub.status.idle": "2022-10-24T16:50:29.834018Z",
     "shell.execute_reply": "2022-10-24T16:50:29.832936Z",
     "shell.execute_reply.started": "2022-10-24T16:49:52.351166Z"
    },
    "tags": []
   },
   "source": [
    "<a id=\"dataset\"></a>\n",
    "### Enregistrement du dataset\n",
    "\n",
    "Sont enregistrés en sortie dans le répertoire \"data_output\" du projet \"ocp8project\" :\n",
    "- Le dictionnaire des labels de fruit au format json:  labels.json ;\n",
    "- Le dataframe des labels du dataset au format parquet: y.parquet ;\n",
    "- Le dataframe des features du dataset au format parquet: X.parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a3cb0b-54fa-4e5e-9b26-1612a72a2d9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:45:50.108057Z",
     "iopub.status.busy": "2022-10-28T12:45:50.107891Z",
     "iopub.status.idle": "2022-10-28T12:47:31.866519Z",
     "shell.execute_reply": "2022-10-28T12:47:31.865707Z",
     "shell.execute_reply.started": "2022-10-28T12:45:50.108035Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fa2d7aedf441f9ab2d665c2e099891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde du dataset constitu? de 90 images de 3 fruits, vectoris?es en 1280 features\n",
      "Sauvegarde du dictionnaire des labels dans s3://ocp8project/output_data/labels.json\n",
      "Sauvegarde du dataset-labels au format '.parquet' dans s3://ocp8project/output_data/y.parquet\n",
      "Sauvegarde du dataset-features au format '.parquet' dans s3://ocp8project/output_data/X.parquet\n",
      "9016"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(f\"Sauvegarde du dataset constitué de {img_count} images de {label_count}\"\n",
    "      f\" fruits, vectorisées en {features_size} features\")\n",
    "labels_file = 'labels.json'\n",
    "with open(labels_file, \"w\") as file:\n",
    "    json.dump(labels, file)\n",
    "print(f\"Sauvegarde du dictionnaire des labels dans \"\n",
    "      f\"s3://{AWS_S3_BUCKET}/{OUTPUT_DATA_FOLDER}{labels_file}\")\n",
    "s3_client.upload_file(labels_file, AWS_S3_BUCKET,\n",
    "                      OUTPUT_DATA_FOLDER + labels_file)\n",
    "\n",
    "if not LOCAL:\n",
    "    print(f\"Sauvegarde du dataset-labels au format '.parquet' dans \"\n",
    "          f\"s3://{AWS_S3_BUCKET}/{OUTPUT_DATA_FOLDER}y.parquet\")\n",
    "    y.write.mode('overwrite').parquet('s3://' + AWS_S3_BUCKET + '/' +\n",
    "                                      OUTPUT_DATA_FOLDER + 'y.parquet')\n",
    "    print(f\"Sauvegarde du dataset-features au format '.parquet' dans \"\n",
    "          f\"s3://{AWS_S3_BUCKET}/{OUTPUT_DATA_FOLDER}X.parquet\")\n",
    "    X.write.mode('overwrite').parquet('s3://' + AWS_S3_BUCKET + '/' +\n",
    "                                      OUTPUT_DATA_FOLDER + 'X.parquet')\n",
    "\n",
    "# Nettoyage mémoire\n",
    "import gc\n",
    "del y, img\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7292be1c-1fe8-4d7b-bb06-250fe3a7d47f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:50:54.907958Z",
     "iopub.status.busy": "2022-10-24T16:50:54.907728Z",
     "iopub.status.idle": "2022-10-24T16:51:08.257601Z",
     "shell.execute_reply": "2022-10-24T16:51:08.256607Z",
     "shell.execute_reply.started": "2022-10-24T16:50:54.907932Z"
    },
    "tags": []
   },
   "source": [
    "*[retour SOMMAIRE](#toc)*\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5830be3f-cede-46b4-ab06-2454964b65ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:50:54.907958Z",
     "iopub.status.busy": "2022-10-24T16:50:54.907728Z",
     "iopub.status.idle": "2022-10-24T16:51:08.257601Z",
     "shell.execute_reply": "2022-10-24T16:51:08.256607Z",
     "shell.execute_reply.started": "2022-10-24T16:50:54.907932Z"
    },
    "tags": []
   },
   "source": [
    "<a id=\"standardization\"></a>\n",
    "### Standardisation des features de X\n",
    "\n",
    "Avant de procéder à une réduction de dimension avec PCA, les features sont standardisées et la fonction de mise à l'échelle enregistrée dans le répertoire \"data_output\" du projet \"ocp8project\" avec la fonction 'save' de pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21cc9030-33c4-4cd5-9eb4-d6ad5a879a56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:47:31.868220Z",
     "iopub.status.busy": "2022-10-28T12:47:31.867866Z",
     "iopub.status.idle": "2022-10-28T12:48:27.352303Z",
     "shell.execute_reply": "2022-10-28T12:48:27.351639Z",
     "shell.execute_reply.started": "2022-10-28T12:47:31.868183Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025e50aa76ff45fc8e82c1fd72216fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mise ? l'?chelle des features avec StandardScaler:\n",
      "Donn?es X mises ? l'?chelle avec StandardScaler: X:90 lignes * 1 colonne\n",
      "Sauvegarde du scaler dans s3://ocp8project/output_data/std_scaler\n",
      "123"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "print(f\"Mise à l'échelle des features avec StandardScaler:\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\").fit(X)\n",
    "X = scaler.transform(X).select('scaledFeatures')\n",
    "print(\n",
    "    f\"Données X mises à l'échelle avec StandardScaler: X:{X.count()} lignes * {len(X.columns)} colonne\"\n",
    ")\n",
    "\n",
    "# Sauvegarde de scaler et nettoyage mémoire\n",
    "scaler_file = 'std_scaler'\n",
    "if LOCAL:\n",
    "    scaler.write().overwrite().save(scaler_file)\n",
    "else:\n",
    "    print(\n",
    "        f\"Sauvegarde du scaler dans s3://{AWS_S3_BUCKET}/{OUTPUT_DATA_FOLDER}{scaler_file}\"\n",
    "    )\n",
    "    scaler.write().overwrite().save('s3://' + AWS_S3_BUCKET + '/' +\n",
    "                                    OUTPUT_DATA_FOLDER + scaler_file)\n",
    "\n",
    "del scaler\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e275f-b1da-4bb4-b7d8-6a2d54bae164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:51:32.903887Z",
     "iopub.status.busy": "2022-10-24T16:51:32.903665Z",
     "iopub.status.idle": "2022-10-24T16:52:02.433528Z",
     "shell.execute_reply": "2022-10-24T16:52:02.432804Z",
     "shell.execute_reply.started": "2022-10-24T16:51:32.903863Z"
    },
    "tags": []
   },
   "source": [
    "*[retour SOMMAIRE](#toc)*\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059bd8a4-655a-4d67-82d6-8af4730ff31a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:51:32.903887Z",
     "iopub.status.busy": "2022-10-24T16:51:32.903665Z",
     "iopub.status.idle": "2022-10-24T16:52:02.433528Z",
     "shell.execute_reply": "2022-10-24T16:52:02.432804Z",
     "shell.execute_reply.started": "2022-10-24T16:51:32.903863Z"
    },
    "tags": []
   },
   "source": [
    "<a id=\"pca\"></a>\n",
    "### Réduction de dimension avec un PCA\n",
    "\n",
    "Le jeu de données X contenant les features est réduit avec un PCA et enregistré dans le répertoire \"data_output\" du projet \"ocp8project\" au format parquet: X_pca.parquet.\n",
    "Le nombre de features résultantes est fixé à 1/10ème du nombre de feature initial, ce qui est largement suffisant pour les 3 * 30 images de ce projet, mais sera à revoir lors d'une mise à l'échelle, en fonction de la variance expliquée.\n",
    "La variance expliquée totale est affichée et son vecteur enregistré pour permettre l'analyse en fonction du nombre d'images et ajuster en conséquence la réduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "160a681e-5b45-4eb8-b69c-3b6776237eae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:48:27.354669Z",
     "iopub.status.busy": "2022-10-28T12:48:27.354454Z",
     "iopub.status.idle": "2022-10-28T12:50:25.158965Z",
     "shell.execute_reply": "2022-10-28T12:50:25.158127Z",
     "shell.execute_reply.started": "2022-10-28T12:48:27.354645Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6175b55a26b4a8c9f28f644a3f2a320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R?duction de dimension de X avec PCA (k=128):\n",
      "Dataset r?duit (X):90 lignes * 1 colonne"
     ]
    }
   ],
   "source": [
    "# Réduction de dimension de X avec PCA\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "n_features = max(2, max_feat // 10) if DEBUG else features_size // 10\n",
    "print(f\"Réduction de dimension de X avec PCA (k={n_features}):\")\n",
    "pca = PCA(k=n_features, inputCol='scaledFeatures',\n",
    "          outputCol='pcaFeatures').fit(X)\n",
    "X = pca.transform(X).select('pcaFeatures')\n",
    "print(f\"Dataset réduit (X):{X.count()} lignes * {len(X.columns)} colonne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d5e41ea-af52-44d0-abcd-7afa30c5474e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:50:25.160432Z",
     "iopub.status.busy": "2022-10-28T12:50:25.160230Z",
     "iopub.status.idle": "2022-10-28T12:51:08.592416Z",
     "shell.execute_reply": "2022-10-28T12:51:08.591774Z",
     "shell.execute_reply.started": "2022-10-28T12:50:25.160404Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff9333a84054ed99b9469d02e965c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde du dataset-features r?duit dans s3://ocp8project/output_data/Xpca.parquet\n",
      "253"
     ]
    }
   ],
   "source": [
    "# Écriture de X dans S3\n",
    "if not LOCAL:\n",
    "    print(f\"Sauvegarde du dataset-features réduit dans \"\n",
    "          f\"s3://{AWS_S3_BUCKET}/{OUTPUT_DATA_FOLDER}Xpca.parquet\")\n",
    "    X.write.mode('overwrite').parquet('s3://' + AWS_S3_BUCKET + '/' +\n",
    "                                      OUTPUT_DATA_FOLDER + 'Xpca.parquet')\n",
    "\n",
    "del X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f016ead-4f01-429a-9c9b-834398b86395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:51:08.593857Z",
     "iopub.status.busy": "2022-10-28T12:51:08.593669Z",
     "iopub.status.idle": "2022-10-28T12:51:08.900435Z",
     "shell.execute_reply": "2022-10-28T12:51:08.899503Z",
     "shell.execute_reply.started": "2022-10-28T12:51:08.593835Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7126ba2954347f2b45216dbde600dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance expliqu?e totale: 100.00%\n",
      "Sauvegarde de la variance expliqu?e dans s3://ocp8project/output_data/explained_var.pkl"
     ]
    }
   ],
   "source": [
    "# Variance expliquée\n",
    "import pickle\n",
    "\n",
    "print(f\"Variance expliquée totale: {100*pca.explainedVariance.sum():.2f}%\")\n",
    "explained_var = {\n",
    "    'explained_var_vec': pca.explainedVariance,\n",
    "    'explained_var_vsum': pca.explainedVariance.sum()\n",
    "}\n",
    "explained_var_file = 'explained_var.pkl'\n",
    "\n",
    "with open(explained_var_file, 'wb') as file:\n",
    "    pickle.dump(explained_var, file)\n",
    "print(f\"Sauvegarde de la variance expliquée dans s3://\"\n",
    "      f\"{AWS_S3_BUCKET}/{OUTPUT_DATA_FOLDER}{explained_var_file}\")\n",
    "s3_client.upload_file(explained_var_file, AWS_S3_BUCKET,\n",
    "                      OUTPUT_DATA_FOLDER + explained_var_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4746f804-3ddf-4aef-b848-8d49d72fb2b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:51:08.904347Z",
     "iopub.status.busy": "2022-10-28T12:51:08.903759Z",
     "iopub.status.idle": "2022-10-28T12:51:11.238116Z",
     "shell.execute_reply": "2022-10-28T12:51:11.237203Z",
     "shell.execute_reply.started": "2022-10-28T12:51:08.904303Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027f7c711d204b1fa30210f558e174f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde du PCA dans s3://ocp8project/output_data/pca\n",
      "204"
     ]
    }
   ],
   "source": [
    "# Sauvegarde PCA et nettoyage mémoire\n",
    "pca_file = 'pca'\n",
    "if LOCAL:\n",
    "    pca.write().overwrite().save(pca_file)\n",
    "else:\n",
    "    print(f\"Sauvegarde du PCA dans s3://{AWS_S3_BUCKET}/{OUTPUT_DATA_FOLDER}{pca_file}\")\n",
    "    pca.write().overwrite().save('s3://' + AWS_S3_BUCKET + '/' + OUTPUT_DATA_FOLDER + pca_file)\n",
    "\n",
    "del pca\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "707a4c6d-9031-4548-b82f-1cce933d9fac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-28T12:51:11.239506Z",
     "iopub.status.busy": "2022-10-28T12:51:11.239327Z",
     "iopub.status.idle": "2022-10-28T12:51:11.440358Z",
     "shell.execute_reply": "2022-10-28T12:51:11.439423Z",
     "shell.execute_reply.started": "2022-10-28T12:51:11.239483Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a2a6764f884c40ab2f6a2d9312df53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset spark configuration\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\",\n",
    "               prev)  # Restaure conf précédente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f3e90c-1555-4dcb-80c5-246d0528c5a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T16:52:43.908097Z",
     "iopub.status.busy": "2022-10-24T16:52:43.907875Z",
     "iopub.status.idle": "2022-10-24T16:52:43.975129Z",
     "shell.execute_reply": "2022-10-24T16:52:43.974353Z",
     "shell.execute_reply.started": "2022-10-24T16:52:43.908073Z"
    },
    "tags": []
   },
   "source": [
    "*[retour SOMMAIRE](#toc)*\n",
    "\n",
    "&nbsp;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
